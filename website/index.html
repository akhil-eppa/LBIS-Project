<!DOCTYPE html>
<html lang="en">
<head>
  <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  <script>
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
  </script>
  </head>
<title>16-726 Final Project</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>

body {
  font-family: Arial, Helvetica, sans-serif;
}
<!--
* {
  box-sizing: border-box;
}
-->

.column {
  float: left;
  width: 50.00%;
  padding: 5px;
}

.column4 {
  float: left;
  width: 25.00%;
  padding: 2px;
}

.column5 {
  float: left;
  width: 20.00%;
  padding: 2px;
}

.column6 {
  float: left;
  width: 16.66%;
  padding: 2px;
}

.column3 {
  float: left;
  width: 33.33%;
  padding: 10px;
}

.column2 {
  float: left;
  width: 50%;
  padding: 10px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}

.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
/* Style the header */
header {
  background-color: #58AADD;
  padding: 30px;
  text-align: center;
  font-size: 35px;
  color: white;
}

article {
  float: left;
  align-content: center;
  padding: 100px;
  background-color: white;

}

/* Clear floats after the columns */
section::after {
  content: "";
  display: table;
  clear: both;
}

/* Style the footer */
footer {
  background-color: #58AADD;
  padding: 10px;
  text-align: center;
  color: white;
}

/* Responsive layout - makes the two columns/boxes stack on top of each other instead of next to each other, on small screens */
@media (max-width: 600px) {
  nav, article {
    width: 100%;
    height: auto;
  }
}
</style>
</head>
<body>


<header>
  <h4>16-726 Learning Based Image Synthesis Final Project</h4>
  <h2>Ancient to Modern Photos using GANs</h2>
  <h5>Akhil Eppa (aeppa@andrew.cmu.edu), Roshini Rajesh Kannan (rrajeshk@andrew.cmu.edu), Sanjana Moudgalya (smoudgal@andrew.cmu.edu)</h5>
</header>

<section>

  
  <article>
    <h2 align="center">Introduction</h2>

    Image restoration has seen a great deal of progress, with the creation of contemporary photographs using 
    filters like denoising, colorization, and super resolution on old, grainy, black-and-white photos. 
    In addition to this, the creators of Time-Travel Rephotography employ StyleGAN2 to transpose outdated 
    images into a contemporary high-resolution image space. In order to imitate the characteristics of 
    vintage cameras and the aging process of film, they use a physically based film deterioration operator. 
    They eventually create the model output image using contextual loss and color transfer loss. 
    The process of converting ancient photos to a modern versions gives the audience a perspective 
    of how someone would have looked during the time and helps revisualize the aspect of color as well.

    <hr>
    <hr>
    <h2 align="center">Problem Description</h2>
    <p>

    </p>
    <hr>
    <hr>
    <h2 align="center">Scope for Improvements</h2>
    <p>
    Based on our review of the work  <a href="https://arxiv.org/pdf/2012.12261.pdf" target="_blank">Time Travel Rephotography</a> we decided to experiment 
    with a few potential improvements that we can try out in order to improve the quality of the images. The scopes 
    for improvement are as listed below: 
    <br>
    <div class="center">
    <ul style="list-style-type:circle" >
      <li>Reduce effect of brightness and contrast of the input image on the predicted skin color : We experiment 
        with using Huber Loss instead of Smooth L1 Loss. 
      </li>
      <br>
      <li>Improve inaccurately predicting skin texture from images in certain cases : We experiment with a preprocessing
        strategy and different loss combinations. We also try out noise regularization and changing its hyperparameters. 
      </li>
      <br>
      <li>Utilizing L2 Loss for reconstruction instead of L1 Loss : In order to better capture the finer details, we implement
        the L2 Loss instead of the L1 Loss for the reconstruction and determine whether there is an improvement. </li>
      <br>
        <li>
        Using a preprocessing pipeline to remove artifacts: We noticed that accessories present around people's faces 
        such as hats and extensive beards tend to be not reconstructed correctly. Hence we try out a preprocessing 
        strategy based on an attention based GAN in order to capture these details better. 
      </li>
      <br>
      <li>
        Improve the strength of noise regularizer: We came up with different ways of reducing the noise regularizer strength that optimizes the w+ global style codes in an aim to focus more on local features. 
      </li>
      <br>
      <li>
        Using a better interpolation method: We experimented with a bicubic interpolation method instead of bilinear to see if it provides more finer details. 
      </li>
    </ul>
  </div>
    </p>
    <hr>
    <hr>
    <h2 align="center">Examples of Bad Results</h2>
    <p>
      
    </p>
    <hr>
    <hr>
    <h2 align="center">Perceptual Loss: Using L2 Loss instead of L1 Loss</h2>
    <p>
    An important component of the Image Reconstruction Loss is the Perceptual Loss. In the base implementation of the 
    paper <a href="https://arxiv.org/pdf/2012.12261.pdf" target="_blank">Time Travel Rephotography</a>, L1 Loss is used 
    in the perceptual loss component. But as we saw in the failure cases, certain fine grained details are not well preserved. 
    There is a loss in the fine details such as the wrinkles and the skin textures which does not make the reconstruction look
    accurate. In order to improve this aspect, we experiment with L2 Loss in the reconstruction and see that the final 
    reconstructed image that is of better qaulity and the finer details are preserved better. <br>
    L1 Loss can be defined as follows:
    $$L1 Loss Function = \frac{\sum_{i=1}^{n} \left | y_{true} - y_{pred} \right |}{n} $$
    <br>
    <br>
    Similarly we can define the L2 Loss function as follows: <br>
    $$L2 Loss Function = \frac{\sum_{i=1}^{n} \left ( y_{true} - y_{pred} \right )^{2}}{n}$$

    <br>
    <br>
    In L1 Loss we take the mean absolute difference while in case of L2 Loss we take the mean squared error. 
    Both these loss functions are used commonly when it comes to image synthesis tasks and for reconstruction 
    purposes. The L2 loss is more sensitive to outliers than the L1 loss, meaning that it gives more weight to large errors. 
    This can be useful in situations where we want to penalize large errors more severely. 
    <br>
    In our experimentation we saw that L2 Loss gave better attention to detail than L1 loss did. The use of L2 Loss 
    certainly provided us an improvement over using L1 Loss. <br>
    Let us take a look at few examples below. <br>
    <br>
    In this first example, let us concentrate on the areas demarcated by the red rectangles. If you compare the 
    content within the red rectangles for the original image, the image generated with L1 Loss and the image generated with L2 loss,
    we see that details with L2 loss are much more finer and sharper and better preserved generally during the reconstruction. 
    As you can see, the hair details and the beard details are much more sharper when L2 Loss is used rather than  
    L1 Loss. Additionally the texture in the wrinkles specifically between the eyebrows and the forehead is much better 
    preserved than when L1 Loss was used. Thus we can say that L2 loss certainly provided an improvement over the L1 Loss. 
    
    <div class="row">
        <div class="column3">
            <p align="center">Original Old Image</p>
        </div>
        <div class="column3">
            <p align="center">Image Reconstructed with L1 Loss</p>
        </div>
        <div class="column3">
            <p align="center">Image Reconstructed with L2 Loss</p>
        </div>
    </div>
    <div class="row">
        <div class="column3">
          <img src="images/l2_loss_markings/old.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column3">
          <img src="images/l2_loss_markings/l1_loss.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column3">
          <img src="images/l2_loss_markings/l2_loss.png" alt="16726 Image" style="width:60%", class="center">
        </div>
    </div>

    <br>
    <br>
    A few more examples are shown below in order to demonstrate that L2 Loss generalises well to other images as well and this is 
    not a one of case of improvement. The examples are as shown below. 
    <div class="row">
        <div class="column3">
            <p align="center">Original Old Image</p>
        </div>
        <div class="column3">
            <p align="center">Image Reconstructed with L1 Loss</p>
        </div>
        <div class="column3">
            <p align="center">Image Reconstructed with L2 Loss</p>
        </div>
    </div>
    <div class="row">
        <div class="column3">
          <img src="images/old_images/old_1.jpeg" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column3">
          <img src="images/with_l1_loss/old_1.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column3">
          <img src="images/with_l2_loss_both/old_1.png" alt="16726 Image" style="width:60%", class="center">
        </div>
    </div>

    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L1 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
  </div>
  <div class="row">
      <div class="column3">
        <img src="images/old_images/old_2.jpeg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l1_loss/old_2.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l2_loss_both/old_2.png" alt="16726 Image" style="width:60%", class="center">
      </div>
  </div>

  <div class="row">
    <div class="column3">
        <p align="center">Original Old Image</p>
    </div>
    <div class="column3">
        <p align="center">Image Reconstructed with L1 Loss</p>
    </div>
    <div class="column3">
        <p align="center">Image Reconstructed with L2 Loss</p>
    </div>
</div>
<div class="row">
    <div class="column3">
      <img src="images/old_images/old_3.jpeg" alt="16726 Image" style="width:60%", class="center">
    </div>
    <div class="column3">
      <img src="images/with_l1_loss/old_3.png" alt="16726 Image" style="width:60%", class="center">
    </div>
    <div class="column3">
      <img src="images/with_l2_loss_both/old_3.png" alt="16726 Image" style="width:60%", class="center">
    </div>
</div>

<div class="row">
  <div class="column3">
      <p align="center">Original Old Image</p>
  </div>
  <div class="column3">
      <p align="center">Image Reconstructed with L1 Loss</p>
  </div>
  <div class="column3">
      <p align="center">Image Reconstructed with L2 Loss</p>
  </div>
</div>
<div class="row">
  <div class="column3">
    <img src="images/old_images/old_5.jpeg" alt="16726 Image" style="width:60%", class="center">
  </div>
  <div class="column3">
    <img src="images/with_l1_loss/old_5.png" alt="16726 Image" style="width:60%", class="center">
  </div>
  <div class="column3">
    <img src="images/with_l2_loss_both/old_5.png" alt="16726 Image" style="width:60%", class="center">
  </div>
</div>
<br>
As we can see from all the above examples, L2 Loss has clearly provided a better result overall than L1 Loss. This is one
improvement we found out during the course of experimentation for this project. 

    </p>
    <hr>
    <hr>
    <!--****************************************************************************************************-->
    <h2 align="center">Preprocess Image with Attention Based GAN</h2>
    <p>
    When we analyzed the shortcomings of the current approach we noticed that certain artifacts in the image such as 
    the accessories a person was wearing were not clear in the generated image. In order to tackle this problem we decided to
    experiment with an Attention Based GAN as a preprocessing step. 
    <br>
    <br>
    In conventional GANs, the generator produces an image by taking a random noise vector as input and 
    transforming it using a number of convolutional layers and upsampling. The discriminator then assesses
     the created image and determines whether it is authentic or phony by comparing it to the original.
    <br>
    <br>
    In an attention-based GAN, the generator architecture includes a self-attention mechanism that enables the 
    generator to deliberately concentrate on key areas of the image as it is being created. A learnable attention 
    map that is created from the intermediate feature maps of the generator is used to implement the self-attention 
    mechanism. The feature maps are then weighted using this attention map, giving the crucial areas of the image 
    greater attention. This is the primary reason we decided to experiment with attention based GANs for the 
    preprocessing step. In our preprocesing step we use the <a href="https://github.com/jantic/DeOldify" target="_blank">DeOldify</a> pipeline and this preprocessed image is passed as 
    input to the modified reconstruction pipeline that involves the L2 loss in the reconstruction Loss instead of the L1 Loss. The entire pipeline is as shown below:
    <br>
    <br>
    <img src="images/pipeline/Pipeline.png" alt="16726 Image" style="width:60%", class="center">
    <br>
    <br>
    Let us now look at the a specific case where there is drastic improvement in the structure and the sharpness of the 
    artifiacts such as the cap and the specific beard shape when the preprocessing is performed before passing the 
    preprocessed image through the reconstruction pipeline. In the given example below notice the areas within the 
    red rectangles. When we analyze this example, we see that the structure of the cap is much better preserved 
    when the preprocessing is done on the image. The chequered pattern in the hat is more clearly preserved in the reconstructed
    image. Additionally when we look at the structure of the moustache, we see that the overall shape and sharpness of the 
    moustache is better preserved overall. Hence, the preprocessing pipeline made out of the Attention-Based GAN certainly
    improved the quality of the regenerated image. 

    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Preprocessed Image Reconstructed with L2 Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/Att_gan_markings/old.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/Att_gan_markings/l2_loss.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/Att_gan_markings/l2_loss_preprocess.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>

    In order to show that this improvement in the quality of the images is not isolated to specific set of images, 
    the experiment is also run on a bunch of other images and even in these cases an improvement is seen when 
    the image is preprocessed before passing it through the reconstruction pipeline. We are hence able to generalize
    that this improvment works across a variety of images and is clearly not isolated to a particular type of an image.  
    <br>
    <br>

    In the below example it can seen how the wrinkles become more accurate and finer when the preprocessing is 
    done rather then when there is no preprocessing done. 
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Preprocessed Image Reconstructed with L2 Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/old_images/abraham.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l2_loss_both/abraham.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/l2_both_and_preprocess/abraham.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    <br>
    <br>
    In the below example, again we can see how the wrinkles are more accurate and also the hair pattern is clearly
    much more accurate in comparison to not doing the preprocessing. Hence we can see that preprocessing the image 
    clearly provides an update here as well. 
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Preprocessed Image Reconstructed with L2 Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/old_images/old_1.jpeg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l2_loss_both/old_1.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/l2_both_and_preprocess/old_1.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    <br>
    <br>
    In the example below, both the wrinkles and the hat that the man is wearing become more clear when the processing 
    is done. Without the preprocesing it seemed like the hat had the texture of hair but when the preprocessed image 
    is used, the hat texture more closely resembles the input image. Furthermore, the wrinkles are also much more well 
    defined than when no preprocessing was performed. Once again an improvement is seen in case of this image as well. 
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Preprocessed Image Reconstructed with L2 Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/old_images/old_2.jpeg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l2_loss_both/old_2.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/l2_both_and_preprocess/old_2.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    <br>
    <br>
    Finally in the example below we can see how the wrinkles are better preserved and also the shape of the eyes 
    is preserved better when the image is preprocessed rather than when there is no preprocessing that is 
    performed. Overall all these examples showcase that preprocesing does improve the quality of the final 
    reconstructed image. 
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with L2 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Preprocessed Image Reconstructed with L2 Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/old_images/old_5.jpeg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/with_l2_loss_both/old_5.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/l2_both_and_preprocess/old_5.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>

  </p>
    <hr>
    <hr>
    <!--****************************************************************************************************-->
    <h2 align="center">Color Transfer Loss: Using Huber Loss instead of Smooth L1 Loss</h2>
    <p>
    In this particular proposal, we try to use Huber Loss instead of Smooth L1 Loss in order to see 
    whether modifying the loss function in this way would improve the colors in the generated image or not. Though 
    Huber Loss and Smooth L1 Loss are quite similar, there are subtle differences which need to be discussed. Let us first look 
    at the formulae for both Huber Loss and Smooth L1 Loss. 
    <br>
    <br>
    Smooth L1 Loss is explained as the following: 
    $$ l\left (x, y  \right ) = L = \left \{ l_{1}, ... ,l_{n} \right \}^{T}
    \\
    \\
    l_{n} = \begin{cases}
          0.5\left ( x_{n} - y_{n} \right )^{2} / beta & if \left | x_{n} - y_{n} \right | < beta\\
          \left | x_{n} - y_{n} \right | - 0.5 * beta & ,otherwise\\
        \end{cases} 
    $$
    <br>
    <br>
    Huber Loss is explained as the following:
    $$
    l\left (x, y  \right ) = L = \left \{ l_{1}, ... ,l_{n} \right \}^{T}
    \\
    \\
    \\
    l_{n} = \begin{cases}
          0.5\left ( x_{n} - y_{n} \right )^{2} & if \left | x_{n} - y_{n} \right | < delta\\
          delta * \left (\left | x_{n} - y_{n} \right | - 0.5 * delta  \right ) & ,otherwise\\
        \end{cases}  
    $$

    <br>
    <br>

    Smooth L1 Loss is quadratic for small errors and linear for large errors. That said, it behaves in a manner
    similar to MSE loss for small errors but it becomes less sensitive to outliers than MSE loss when the errors 
    are large. 
    On the other hand, Huber Loss is a compromise between MSE Loss and the Mean Absolute Error Loss (MAE) loss. 
    For small errors, Huber Loss behaves like MSE loss, while for large errors, it behaves like MAE loss. 
    This makes Huber Loss less sensitive to outliers than MSE loss while maintaining its differentiability. 
    In our case, we test with Huber Loss with delta parameter set as 0.5. 
    <br>
    <br>
    Let us take a look at an example below. As we can see from the example below, using Huber Loss over Smooth L1 loss
    does not provide drastic improvement in the color of the generated image. Rather it looks a little washed out 
    when we make use of Huber Loss instead of Smooth L1 Loss. Hence this proposal does not provide us with a great improvement
    as anticipated. From this we can conclude that it is beneficial to stick with Smooth L1 Loss instead of going 
    with Huber Loss. 
    <br>
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with Smooth L1 Loss</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with Huber Loss</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/old_images/old_1.jpeg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/color_transfer/smooth_l1.png" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/color_transfer/huber.png" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    </p>
    <hr>
    <hr>
    <!--****************************************************************************************************-->
    <h2 align="center">Noise Regularizer</h2>
    <p>
    During the image synthesis process, the global w+ style codes(StyleGAN2 network's learned style vectors) are optimized with a strong noise regularizer.
    By a strong noise regularizer, we mean that a regularization technique is used to control the noise input to the StyleGAN2 network.
    This ensures that the noise input is not carrying any significant signal and is only used to introduce controlled randomness to the image synthesis process.
    So, having a strong regularizer for global w+ style codes might make it hard to preserve local image details, textures, etc.
    We experimented with two different ways of reducing the strength of this noise regularizer. One was to just reduce the weight of the noise regularizer loss and the 
    other way was to reduce the dimension until which the loss is calculated.
    <h3 align="center">Reducing the weight</h3>
    We experimented with a smaller weight for the noise regularizer loss of 5000 as opposed to the default value of 50000. But in the below images we see that this does not work and makes the reconstructed image different from the original old image.
    So, an alternate method to reduce the strength of the regularizer had to be found.
    <br>
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with noise regularizer weight 50000</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with noise regularizer weight 5000</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/noise_reg/weight/input.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/weight/before.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/weight/after.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    </p>
   

    <h3 align="center">Reducing the pyramid resolution</h3>
    <p>
      The regularization is performed on noise maps at multiple resolution scales. 
      A pyramid structure is created by downsampling the original noise map at different resolutions, with each level 
      being downsampled by averaging 2x2 pixel neighborhoods and multiplying the result by 2 to maintain the expected unit variance.
      The purpose of the pyramid structure is to create a set of downsampled noise maps that can be used for regularization without affecting 
      the actual image synthesis process. These downsampled noise maps are only used to compute the regularization loss during training and do not play a
      role in the final image synthesis. <br /><br />Overall, the pyramid downsampled noise maps are used to regularize the noise maps at different resolutions, 
      providing a smoother and more stable signal that can be used to synthesize high-quality images.
      This pyramid structure is created by taking each noise map that is greater than 8x8 in size in the original method.
      To reduce the strength we take noise maps with resolution just greater than 16x16 opposed to 8x8 in original method.
      By this method, we were able to get improvements in preserving the basic structure of the input image in comparison to having a strong regularizer.
      <br>
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with strong noise regularizer</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with weak noise regularizer</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/noise_reg/strength/input.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/strength/before.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/strength/after.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/noise_reg/strength/input2.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/strength/before2.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/noise_reg/strength/after2.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
    In the above images we see that the ear of Abraham Lincoln and lips of Mahatma Gandhi look similar to the input image when we reduce the strength of the regularizer.
    
    </p>
    <hr>
    <hr>
    <h2 align="center">Bicubic interpolation instead of bilinear</h2>
    <p>
    During resizing processes in the image synthesis process, Bilinear interpolation is used. We experimented by substituting this with Bicubic interpolation. This gave outputs with more finer details such as wrinkles preserved as shown in the image below.
    This happens because Bicubic interpolation uses a larger window of neighboring pixels compared to bilinear and hence, more finer details are preserved.
    <br />Given a grid of values f(i,j), where i and j are integer indices, and a point (x,y) in between the grid points, the bilinear interpolation formula for estimating the value f(x,y) is:
    <p align="center">f(x,y) = f(i,j)(1-u)(1-v) + f(i+1,j)u(1-v) + f(i,j+1)(1-u)v + f(i+1,j+1)uv</p>
    where i = floor(x), j = floor(y), u = x-i, and v = y-j.
    <br/>
    Given a grid of values f(i,j), where i and j are integer indices, and a point (x,y) in between the grid points, the bicubic interpolation formula for estimating 
    the value f(x,y) is:
    <p align="center">f(x,y) = ∑<sub>i=0</sub><sup>3</sup> ∑<sub>j=0</sub><sup>3</sup> a<sub>i,j</sub> * x<sup>i</sup> * y<sup>j</sup></p> 
         where a(i,j) are the coefficients of the cubic function, which are determined by solving a system of linear equations using the values of the grid points and their derivatives.
    <div class="row">
      <div class="column3">
          <p align="center">Original Old Image</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with bilinear interpolation</p>
      </div>
      <div class="column3">
          <p align="center">Image Reconstructed with bicubic interpolation</p>
      </div>
    </div>
    <div class="row">
      <div class="column3">
        <img src="images/bicubic/input.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/bicubic/before.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
      <div class="column3">
        <img src="images/bicubic/after.jpg" alt="16726 Image" style="width:60%", class="center">
      </div>
    </div>
      
  </p>
    <hr>
    <hr>
    <h2 align="center">Discussion</h2>
    <p>
      
    </p>
    <hr>
    <hr>
    <h2 align="center">Conclusion and Future Work</h2>
    <p>
      
    </p>
    <hr>
    <hr>
    <!--
    <div class="row">
        <div class="column6">
            <p align="center">Original Image</p>
        </div>
        <div class="column6">
            <p align="center">Perceptual Loss Weight 0.1</p>
        </div>
        <div class="column6">
            <p align="center">Perceptual Loss Weight 0.01</p>
        </div>
        <div class="column6">
          <p align="center">Perceptual Loss Weight 0.001</p>
      </div>
      <div class="column6">
          <p align="center">Perceptual Loss Weight 0.0001</p>
      </div>
      <div class="column6">
          <p align="center">Perceptual Loss Weight 0.00001</p>
      </div>
    </div>
    <div class="row">
        <div class="column6">
          <img src="images/perc_weights/0_data.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column6">
          <img src="images/perc_weights/0_stylegan_w_l1_10_0.1_2000.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column6">
          <img src="images/perc_weights/0_stylegan_w_l1_10_0.01_2000.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column6">
          <img src="images/perc_weights/0_stylegan_w_l1_10_0.001_2000.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column6">
          <img src="images/perc_weights/0_stylegan_w_l1_10_0.0001_2000.png" alt="16726 Image" style="width:60%", class="center">
        </div>
        <div class="column6">
          <img src="images/perc_weights/0_stylegan_w_l1_10_1e-05_2000.png" alt="16726 Image" style="width:60%", class="center">
        </div>
    </div>-->
</article>
</section>

<footer>
  <div class="row">
        <div class="column">

          <a href="https://learning-image-synthesis.github.io/sp23/" target="_blank">16-726 Course Website</a>
        </div>
        <div class="column">
        Webpage resources reference: <a href="https://www.w3schools.com/" target="_blank">W3 Schools</a>
        </div>
    </div>

</footer>

</body>
</html>